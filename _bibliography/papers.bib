---
---

@inproceedings{rikters-etal-2017-c,
    title = "{C}-3{MA}: {T}artu-{R}iga-{Z}urich Translation Systems for {WMT}17",
    author = "Rikters, Mat{\=\i}ss  and
      Amrhein, Chantal  and
      Del, Maksym  and
      Fishel, Mark",
    booktitle = "Proceedings of the Second Conference on Machine Translation",
    month = sep,
    year = "2017",
    address = "Copenhagen, Denmark",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W17-4738",
    doi = "10.18653/v1/W17-4738",
    pages = "382--388",
}

@inproceedings{del-etal-2018-phrase,
    title = "Phrase-based Unsupervised Machine Translation with Compositional Phrase Embeddings",
    author = {Del, Maksym  and
      T{\"a}ttar, Andre  and
      Fishel, Mark},
    booktitle = "Proceedings of the Third Conference on Machine Translation: Shared Task Papers",
    month = oct,
    year = "2018",
    address = "Belgium, Brussels",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W18-6407",
    doi = "10.18653/v1/W18-6407",
    pages = "361--367",
    abstract = "This paper describes the University of Tartu{'}s submission to the unsupervised machine translation track of WMT18 news translation shared task. We build several baseline translation systems for both directions of the English-Estonian language pair using monolingual data only; the systems belong to the phrase-based unsupervised machine translation paradigm where we experimented with phrase lengths of up to 3. As a main contribution, we performed a set of standalone experiments with compositional phrase embeddings as a substitute for phrases as individual vocabulary entries. Results show that reasonable n-gram vectors can be obtained by simply summing up individual word vectors which retains or improves the performance of phrase-based unsupervised machine tranlation systems while avoiding limitations of atomic phrase vectors.",
}

@article{DBLP:journals/corr/abs-1903-11283,
  author    = {Elizaveta Korotkova and
               Agnes Luhtaru and
               Maksym Del and
               Krista Liin and
               Daiga Deksne and
               Mark Fishel},
  title     = {Grammatical Error Correction and Style Transfer via Zero-shot Monolingual
               Translation},
  journal   = {CoRR},
  volume    = {abs/1903.11283},
  year      = {2019},
  url       = {http://arxiv.org/abs/1903.11283},
  eprinttype = {arXiv},
  eprint    = {1903.11283},
  timestamp = {Tue, 02 Apr 2019 11:16:55 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1903-11283.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{DBLP:journals/corr/abs-2109-01207,
  author    = {Maksym Del and
               Mark Fishel},
  title     = {Establishing Interlingua in Multilingual Language Models},
  journal   = {CoRR},
  volume    = {abs/2109.01207},
  year      = {2021},
  url       = {https://arxiv.org/abs/2109.01207},
  eprinttype = {arXiv},
  eprint    = {2109.01207},
  timestamp = {Mon, 20 Sep 2021 16:29:41 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2109-01207.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{del-etal-2021-translation,
    title = "Translation Transformers Rediscover Inherent Data Domains",
    author = "Del, Maksym  and
      Korotkova, Elizaveta  and
      Fishel, Mark",
    booktitle = "Proceedings of the Sixth Conference on Machine Translation",
    month = nov,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.wmt-1.65",
    pages = "599--613",
    abstract = "Many works proposed methods to improve the performance of Neural Machine Translation (NMT) models in a domain/multi-domain adaptation scenario. However, an understanding of how NMT baselines represent text domain information internally is still lacking. Here we analyze the sentence representations learned by NMT Transformers and show that these explicitly include the information on text domains, even after only seeing the input sentences without domains labels. Furthermore, we show that this internal information is enough to cluster sentences by their underlying domains without supervision. We show that NMT models produce clusters better aligned to the actual domains compared to pre-trained language models (LMs). Notably, when computed on document-level, NMT cluster-to-domain correspondence nears 100{\%}. We use these findings together with an approach to NMT domain adaptation using automatically extracted domains. Whereas previous work relied on external LMs for text clustering, we propose re-using the NMT model as a source of unsupervised clusters. We perform an extensive experimental study comparing two approaches across two data scenarios, three language pairs, and both sentence-level and document-level clustering, showing equal or significantly superior performance compared to LMs.",
}